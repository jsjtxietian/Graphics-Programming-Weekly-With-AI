2025-11-04: [ONNX and DirectML execution provider guide - part 1](https://gpuopen.com/learn/onnx-directlml-execution-provider-guide-part1/)

- demonstrates how to build native inference pipelines on AMD GPUs using ONNX Runtime with DirectML execution provider
- explains sharing the DirectX 12 context between DirectML and user code to avoid CPU-GPU data transfers
- shows how to map resources directly into ONNX Runtime tensors for efficient preprocessing and postprocessing
